{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304cbfb-2a44-4707-86b0-986f45a93bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found OpenAPI Base Endpoint: \" + os.getenv(\"OPENAI_API_BASE\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "openai_api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "embedding_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "\n",
    "def get_completion(prompt, model=deployment_name, temperature=0.0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1e376-b1e9-411e-9b78-70cb2d62e1c4",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "- **Principle 1: Write clear and specific instructions**\n",
    "- **Principle 2: Give the model time to “think”**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67871f-d848-49dc-8775-d0312f67922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "At four hours by plane, you arrive at the Spanish Gran Canaria, \n",
    "one of the most versatile Canary Islands. Gran Canaria has many \n",
    "attractions such as Roque Nublo and Dunas de Maspalomas. \n",
    "In addition, there are several picturesque villages. So there is \n",
    "always something to do! Curious about the activities you can prepare \n",
    "for? Read all the highlights of Gran Canaria in this blog \n",
    "and let your holiday anticipation begin!\n",
    "\n",
    "Explore attractions with your rental car\n",
    "Gran Canaria is perfect for exploring by car. The GC-60 is the \n",
    "access road to Roque Nublo, a rock formation that is 80 meters high. \n",
    "It is the symbol of the island and is also called the \"Grand Canyon\" \n",
    "of Gran Canaria. It is one of the most beautiful attractions because \n",
    "you can hike and walk among the rugged rocks. So bring good walking \n",
    "shoes. It feels like you're walking on the moon here, so special!\n",
    "\n",
    "Then drive to the mountain village of Fataga. An oasis of tranquility \n",
    "with typical indigenous Canarian architecture: white houses with \n",
    "orange-red roofs and narrow streets. The highlight is the view. \n",
    "On one side, you can see the \"Grand Canyon\" and on the other side, \n",
    "you have a view of the ocean. The capital Las Palmas is also well \n",
    "worth a visit. The city is full of attractions such as a large harbor, \n",
    "cozy markets, and many shopping opportunities. But the city also offers \n",
    "the opportunity to enjoy the beach.\n",
    "\n",
    "Gran Canaria: sun, sea, and beach\n",
    "What to do in Gran Canaria? Relax with the sand between your toes! \n",
    "Every Sunweb travel guide mentions Playa Amadores as one of the \n",
    "most beautiful beaches in Gran Canaria. Of course, you want to \n",
    "see that with your own eyes. Admire the beautiful bay with clear \n",
    "blue water, fine sand, and swaying palm trees. Just the palm trees \n",
    "alone give you that holiday feeling! There are also many restaurants \n",
    "and souvenir shops in Playa Amadores. Near this beach is Puerto Rico, \n",
    "a sunny seaside resort with a beautiful beach and cozy terraces and shops. \n",
    "From here, you can easily reach the charming Puerto de Mogán by car \n",
    "or small ferry. This fishing village has a lively marina with colorful \n",
    "boats and a lovely child-friendly beach. Nice to relax and cool off \n",
    "in the clear water!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks\n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af24a1-1bb6-4bb6-adc9-72afeb781649",
   "metadata": {},
   "source": [
    "#### Tactic 1: Ask for structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a52205-5826-4b98-b2f6-388054887506",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a list of three made-up holiday destinations in Greece \n",
    "with location, average summer temperatures and three main attractions.\n",
    "Provide them in the following format:\n",
    "\n",
    "Destination name:\n",
    "Destination location:\n",
    "Average summer temperature:\n",
    "Three main attractions:\n",
    "    - \n",
    "    -\n",
    "    -\n",
    "\"\"\"\n",
    "response = get_completion(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cab674",
   "metadata": {},
   "source": [
    "**Additional tasks:**\n",
    "\n",
    "* Try adding additional fields to be included or provide additional context to the prompt to generate more contextual output.\n",
    "\n",
    "* Pass temperature parameter to control the randomness of the output. The higher the temperature, the more random the output. (e.g. `get_completion(prompt, temperature=0.9)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713816ba-e91f-40fa-bd3f-f1fd236c370b",
   "metadata": {},
   "source": [
    "#### Tactic 2: Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbcf7a-d9b0-4821-be7c-349c755146a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some\n",
    "water boiling. While that's happening, \n",
    "grab a cup and put a tea bag in it. Once the water is \n",
    "hot enough, just pour it over the tea bag. \n",
    "Let it sit for a bit so the tea can steep. After a \n",
    "few minutes, take out the tea bag. If you \n",
    "like, you can add some sugar or milk to taste.\n",
    "And that's it! You've got yourself a delicious\n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes.\n",
    "If it contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions,\n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b8341-f499-482c-8adc-2dafa3226e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"\"\"\n",
    "Do you want to travel extra comfortably? Or do you want\n",
    "to sit next to your children, for example? Or are you tall\n",
    "and do you like to book extra legroom? Choose and reserve\n",
    "your favorite seat yourself before departure. The seat\n",
    "reservation is provided by the airline you are going on\n",
    "vacation with. Sunweb has no influence on the seat reservation.\n",
    "It is not possible to reserve a seat with all airlines.\n",
    "Below you will find all the airlines we work with and the\n",
    "information you need to reserve a seat.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes.\n",
    "If it contains a sequence of instructions, \\\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\\n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80a098-a351-4186-a4d0-a59dd0a856e4",
   "metadata": {},
   "source": [
    "### Principle 2: Give the model time to “think” \n",
    "\n",
    "#### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a5581-b576-4b69-9ae1-d8dfb7cf4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This special hotel offers you chic rooms & suites that are fully equipped.\n",
    "Each room has its own charm and features+ beautiful oak floors, very comfortable beds\\\n",
    "and luxurious bathrooms. One of the most beautiful parts of the hotel is the spa.\\\n",
    "This fantastic spa is located on the ground floor and from the spa you have an amazing \\\n",
    "view of the mountains and the slopes. Here you will find a sauna, herbal sauna and a Turkish steam bath.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Translate the summary into Dutch.\n",
    "2 - List all hotel characteristics in Dutch.\n",
    "3 - Translate the characteristics into Catalan.\n",
    "4 - List all hotel characteristics in Catalan.\n",
    "5 - Structure output in the following way\n",
    "dutch_summary:\n",
    "dutch_characteristics:\n",
    "catalan_summary:\n",
    "catallan_characteristics:\n",
    "\n",
    "\n",
    "Provide only the output from task number 5\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(\"Completion for prompt 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9438b2-7d95-4fab-ad76-3412c796f7b4",
   "metadata": {},
   "source": [
    "#### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f23083-a026-4314-9919-918a6c10444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financial. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: (100 * x)  + (250 * x) + (100,000 + (100 * x)) = 450x + 100,000\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd683e7-6ff2-43ee-b5d6-1f687d59433f",
   "metadata": {},
   "source": [
    "***Note that the student's solution is actually not correct.***\n",
    "#### We can fix this by instructing the model to work out its own solution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ea70d-3525-430f-ac36-1cfdfc69dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem. \n",
    "- Then compare your solution to the student's solution \\\n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financial. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: (100 * x)  + (250 * x) + (100,000 + (100 * x)) = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3952b53-42f7-4739-b05d-9caa596723f7",
   "metadata": {},
   "source": [
    "## Model Limitations: Hallucinations\n",
    "- As you know Sunweb doesn't offer something called `regenvakantie` but still the model will generate something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e107a-882e-41c6-86dc-9a6eeb6ccdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Tell something about regenvakantie at Sunweb\n",
    "\"\"\"\n",
    "response = get_completion(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7dedd-d096-4f19-863c-06ff89231577",
   "metadata": {},
   "source": [
    "# Iterative Prompt Develelopment\n",
    "In this section, you'll iteratively analyze and refine your prompts to generate marketing copy from a product fact sheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe1e77-561a-41f5-82ab-9b0c590050cb",
   "metadata": {},
   "source": [
    "## Generate a marketing destination description from destination fact sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705cedf4-d1d6-40d7-9600-4f1958a399ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenerife_fact_sheet = \"\"\"\n",
    "\n",
    "- Tenerife is the largest of the Canary Islands.\n",
    "- The Tenerife flag is the same as Scotland’s, this is because St Andrew is the patron saint of the island.\n",
    "- 43% of the entire Canary Islands population live on the island.\n",
    "- The canary bird was named after the islands, not the other way around.\n",
    "- Every year the island attracts over five million tourists.\n",
    "- You will always find a spot to lay your towel with a trip to El Medano – Tenerife’s longest beach stretching out just over two kilometers.\n",
    "- Many of Tenerife’s beaches are not natural, but man-made due to the islands volcanic nature.\\\n",
    "  You will find the natural ones have characteristic back sand.\n",
    "- The popular holiday destination Playa de Las Americas can be reached by both of the international airports:\\\n",
    "  Reina Sofia in the south of the Island and Los Cristianos. \\\n",
    "  Los Rodeos airport north of the island is near the tourist resort of Puerto de La Cruz.\n",
    "- Two of the biggest attractions on the island are Loro Parque in Puerto de la Cruz and the volcano Teide\\\n",
    "  the top of which is more than 3,000 meters above sea level.\n",
    "- Tenerife’s Teide National Park is a UNESCO World Heritage Site and the second most visited park in the entire world.\\\n",
    "- Due to its colossal size, Mount Teide is known to cast the largest sea shadow in the world.\n",
    "- The island’s famous Thai themed Siam Park, is the biggest water park in Europe and offers one of the highest water slides in the world.\n",
    "- The ‘Wind Cave’ (La Cueva del Viento) is the largest volcanic tube in Europe measuring 18 kilometers in length.\\\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a559d-7e5d-4f4a-a587-d03a9ec042cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to help a marketing team \n",
    "of tour operating company create a \n",
    "destination description for the company's website.\n",
    "\n",
    "Write a destination description based on the information \n",
    "provided in the fact sheet delimited by \n",
    "triple backticks.\n",
    "\n",
    "Fact sheet: ```{tenerife_fact_sheet}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444d862-6591-46a6-8ba8-93b82e49e589",
   "metadata": {},
   "source": [
    "## Issue 1: The text is too long \n",
    "- [Action Needed] Update the prompt to limit the number of words/sentences/characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f12354-9dad-4f7b-83a2-e4dc430a825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to help a marketing team \n",
    "of tour operating company create a \n",
    "destination description for the company's website.\n",
    "\n",
    "Write a destination description based on the information \n",
    "provided in the fact sheet delimited by \n",
    "triple backticks.\n",
    "\n",
    "Fact sheet: ```{tenerife_fact_sheet}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876998d-b261-4adf-a410-7777edea4207",
   "metadata": {},
   "source": [
    "## Issue 2. Text focuses on the wrong details\n",
    "- Ask it to focus on the aspects that are relevant to the intended audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7bb65-779b-4580-b5de-af797efa7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to help a marketing team \n",
    "of tour operating company create a \n",
    "destination description for the company's website.\n",
    "\n",
    "Write a destination description based on the information \n",
    "provided in the fact sheet delimited by \n",
    "triple backticks.\n",
    "\n",
    "The description should be written in a way that is attractive\n",
    "for audience looking for active holidays. Write a description\n",
    "in a style of friend sharing recommendation with another friend.\n",
    "Your answer must not be longer than 50 words so choose only top 3\n",
    "recommendations that will be attractive for active holiday seekers. \n",
    "\n",
    "Fact sheet: ```{tenerife_fact_sheet}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecae7c8",
   "metadata": {},
   "source": [
    "**Additional tasks:**\n",
    "\n",
    "* Experiment with prompts that focus on target audience to be elderly people or families with children.\n",
    "* Experiment with prompts that focus on the destination's natural beauty or cultural heritage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe83dd-a94b-4830-bf0d-ff6252691a5a",
   "metadata": {},
   "source": [
    "# Inferring\n",
    "In this section, you will infer sentiment and topics from product reviews and news articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91a5b3-e0b2-4649-b388-5a71eadb281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_one = \"\"\"\n",
    "That girl barely had any product knowledge, preferred to look\\\n",
    "the other way when we meet (they stayed at the hotel).\\\n",
    "Even basic politeness was lacking. In recent years we have noticed more and\\\n",
    "more that there is much less experience and interest in other things than the\\\n",
    "well-being of the customer. After many and many years of Sunweb, \\\n",
    "it's time for us to take a look at the competition. \\\n",
    "Too bad, put a damper on our holiday. Because of Sunweb's standard responses here,\\\n",
    "I'm not going to elaborate either. If necessary, you know where to find us for more feedback.\\\n",
    "Last year we noticed in Gouves, Santa Susanna and now also Rhodes that the well-being\\\n",
    "of the customer and interest in their product no longer come first. Shame!\n",
    "The only positive aspect of this holiday was quality of food in the hotel.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200a8b5-54bc-4c52-ae16-39a38ccd62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_two = \"\"\"\n",
    "Our trip to Crete was booked smoothly from the 1st attempt, after which there was\\\n",
    "good and clear communication for further steps. Upon arrival, Sunweb employees were \\\n",
    "already at the airport for a transfer. The hotel SOLIMAR DIAS was also a good and friendly\\\n",
    "reception with a good location, warm and homely atmosphere. REALLY RECOMMENDED. I will definitely go back\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137af2e2-44b9-45d3-a199-60436eab215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Review text: '''{review_two}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(f'Review Text: {review_two}')\n",
    "print(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31338266-70b0-451c-a209-3ec9fe75ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify a list of emotions that the writer of the \\\n",
    "following review is expressing. Include no more than \\\n",
    "five items in the list. Format your answer as a list of \\\n",
    "lower-case words separated by commas.\n",
    "\n",
    "Review text: '''{review_one}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(f'Review Text: {review_one}')\n",
    "print(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2357663-b252-44cf-a70e-313f74330beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Is the writer of the following review expressing anger?\\\n",
    "The review is delimited with triple backticks. \\\n",
    "Give your answer as either yes or no.\n",
    "\n",
    "Review text: '''{review_two}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(f'Review Text: {review_two}')\n",
    "print(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a0de05-2fac-44f1-861d-f9aaeec75327",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine list of topics that were mentioned in the review.\n",
    "Per each topic, classify if the sentiment associated with it\\\n",
    "is positive, negative or neutral\n",
    "\n",
    "Format your response as a list of items separated by commas.\n",
    "\n",
    "Text sample: '''{review_one}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4901d",
   "metadata": {},
   "source": [
    "# Bring your own data and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162096b-6a87-4f12-a9e9-fce877043862",
   "metadata": {},
   "source": [
    "In this section, we'll explore how we can bring our own data into the models used by Azure OpenAI and introduce [LangChain](https://python.langchain.com/docs/get_started/introduction), a framework for developing applications powered by language models.\n",
    "\n",
    "Langchain supports Python and Javascript / Typescript. For this lab, we will use Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cff306",
   "metadata": {},
   "source": [
    "We'll start by importing the AzureOpenAI specific components from the langchain package, including models and schemas for interacting with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93a228",
   "metadata": {},
   "source": [
    "Next, we'll configure Langchain by providing the API key and endpoint details, along with the API version information.\n",
    "\n",
    "As Langchain can work with multiple AI services, we need to specify that we want to work with Azure via the openai_api_type parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ed3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment_name = deployment_name,\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f96858",
   "metadata": {},
   "source": [
    "Let's begin by asking the AI a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d29b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt we want the AI to respond to - the message the Human user is asking\n",
    "msg = HumanMessage(content=\"Tell me about the latest Ant-Man movie. When was it released? What is it about?\")\n",
    "\n",
    "# Call the AI\n",
    "r = llm(messages=[msg])\n",
    "\n",
    "# Print the response\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59060ec9",
   "metadata": {},
   "source": [
    "What do you notice about the response?\n",
    "\n",
    "The AI thinks the latest \"Ant-Man\" movie was \"Ant-Man and the Wasp\" and it was released in July 2018. This is wrong, as there has been a more recent \"Ant-Man\" movie.\n",
    "\n",
    "OpenAI models are trained on a large set of data, but that happened at a specific point in time depending on the model. So, many of the models have no information about events that took place in recent months or years.\n",
    "\n",
    "To help the AI out, we can provide additional information. This is the same process you would follow if you want the AI to work with your own company data. The AI won't know about information that isn't publically available, so if you want the AI to work with that information, then you'll need to get that information into the model.\n",
    "\n",
    "The thing is, you can't actually do that. The models are pre-trained, so the only way to get more information in is to retrain the model, which is an expensive and time consuming process.\n",
    "\n",
    "However, there are ways to get the AI models to work with new data. The most popular of these methods is to use embeddings, which we'll explore in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc0ff9",
   "metadata": {},
   "source": [
    "### Bring Your Own Data\n",
    "\n",
    "Langchain provides a number of useful tools, which include tools to simplify the process of working with external documents. Below, we'll use the `DirectoryLoader` which can read multiple files from a directory and the `UnstructuredMarkdownLoader` which can process files in Markdown format. We'll use these to process a bunch of markdown formatted files that contain details of movies that were released in the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "data_dir = \"data/movies\"\n",
    "\n",
    "documents = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fffa01",
   "metadata": {},
   "source": [
    "We now have a `documents` object which contains all of the information from our markdown documents about movies.\n",
    "\n",
    "We can use the `question_answering` chain to provide the AI with access to our documents and then ask the same question about Ant-Man movies again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40847e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question answering chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Prepare the chain and the query\n",
    "chain = load_qa_chain(llm)\n",
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "\n",
    "chain.run(input_documents=documents, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a059ba9",
   "metadata": {},
   "source": [
    "Great! The model now knows about the latest Ant-Man movie.\n",
    "\n",
    "However, there's something lurking! Let's take a look at what happened behind the scenes.\n",
    "\n",
    "We'll do two things here. First we'll add the `verbose=True` parameter to the chain, and we'll wrap the chain execution in a callback, which will allow us to capture the number of tokens consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for callbacks\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Prepare the chain and the query\n",
    "chain = load_qa_chain(llm, verbose=True)\n",
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "\n",
    "# Run the chain, using the callback to capture the number of tokens used\n",
    "with get_openai_callback() as callback:\n",
    "    chain.run(input_documents=documents, question=query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641a2c7",
   "metadata": {},
   "source": [
    "In the output from the last code section, you should see a lot of information. At the end, you should see a count of the number of tokens used. You might be surprised to see that the query uses something like 2,500 tokens! That's a lot of tokens.\n",
    "\n",
    "With the verbose option enabled, the rest of the output shows the prompt that was constructed for the query. If you scroll back through the output, you'll see that the prompt included all of the information from our documents, so this is why the query used so many tokens.\n",
    "\n",
    "As we've discussed previously, AI models have a maximum number of tokens you can use and a charging model based on the number of tokens consumed. In this example, the documents are relatively small in size and there's only 20 of them, so clearly this is not going to scale when we want to work with larger documents and more of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6f97e",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The solution to working with large amounts of external information is to use embeddings. OpenAI provide embedding models which allow human readable information to be analysed for meaning and intent. The output from an embedding model is data in a numeric format, known as vectors. These allow computers to group pieces of similar information together. The vectors are then kept in a vector store. When you want to ask a question, an embedding model is again used to convert the query text into vectors and the vector data that represents your query can then be searched in the vector store. Any similar vectors that are found in the database are likely to be a good response to your query.\n",
    "\n",
    "To prevent overloading a prompt with a large number of tokens, instead of sending all of our documents to the AI, we can perform a vector search first to narrow down to a set of interesting results, and then use that smaller subset of information as part of a prompt.\n",
    "\n",
    "Let's walk through the process of using embeddings to give the AI some details about our movies. We'll start by initiating an instance of an embeddings model. You'll notice this is similar to when we initialise one of our model deployments to run a query, but in this case we specify and embedding model. Typically the embedding model used is `text-embedding-ada-002`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec42921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = openai_api_version,\n",
    "    openai_api_base = openai_api_base,\n",
    "    openai_api_key = openai_api_key,\n",
    "    deployment = embedding_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eadbb",
   "metadata": {},
   "source": [
    "Now that we've initialised a model to create embeddings, let's go ahead and embed some documents.\n",
    "\n",
    "As we did in the previous example, we'll use Langchain's builit in loaders to read the documents from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0eeea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64471cb3",
   "metadata": {},
   "source": [
    "The next step is to use a splitter. A splitter enables us to break up larger documents into chunks, so that we don't risk hitting the token limit when submitting our data to the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "document_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044f899",
   "metadata": {},
   "source": [
    "The next stage is to convert the chunks of split documents into vectors which we do by passing the data through an embedding model. The resultant vectors are then stored in a vector database. In this example, we're using the Qdrant (pronounced 'quadrant') database (It's running in a container locally). We initialise it using the `location=\":memory:\"` option, so that the database will be stored in memory rather than persisted to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70990283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    document_chunks,\n",
    "    embeddings_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"movies\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6b823",
   "metadata": {},
   "source": [
    "The above code segment handles the process of initialising the Qdrant database, passing our documents through the embedding model and storing the resulting vectors in the database.\n",
    "\n",
    "Next, we define a retriever. In Langchain, retrievers are an interface that allow results to be returned from vector stores. So, we establish a retriever for our Qdrant database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee5ad1",
   "metadata": {},
   "source": [
    "Next we define a `RetrievalQA` chain. This chain brings handles the process of answering a question by performing the search on the vector store, then taking the results of that search and passing them to our AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da935f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923a39b",
   "metadata": {},
   "source": [
    "Now, we'll run our query again. However, we'll make one small change.\n",
    "\n",
    "You may be thinking that it's not surprising that the AI now knows about the latest Ant-Man movie, because we told it about the latest Ant-Man movie! So, let's try and show that the AI is actually doing some work here, after all it is a reasoning engine.\n",
    "\n",
    "If you're not a fan of these movies, Ant-Man originates from Marvel comic books. And the collection of movies that originate from Marvel comic books are said to be part of the Marvel Cinematic Universe, sometimes referred to as the MCU. We haven't mentioned Marvel or MCU in the data we've provided, so if we modify the query slightly and ask the AI about the MCU instead of specifically about Ant-Man, it should be able to use reasoning to figure out what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33322094",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the latest MCU movie. When was it released? What is it about?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ddbe9",
   "metadata": {},
   "source": [
    "If all went well, the AI should have responded that the latest MCU movie is Ant-Man and the Wasp: Quantumania which was released in February 2023.\n",
    "\n",
    "So, we're getting the response we expected, but let's check in on one of the reasons why we've done all of this. Has the number of tokens used been reduced? Let's use the same technique as before and employ a callback to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71875709",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as callback:\n",
    "    qa.run(query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4b8d",
   "metadata": {},
   "source": [
    "The exact number of tokens used may vary, but it should be clear that this query now uses far fewer tokens than our original query, typically around 2,000 fewer.\n",
    "\n",
    "AI Orchestrators like Langchain and Semantic Kernel can help simplify the process of embedding, vectorization and search. In the preceding section, we stepped through the process of document splitting, embedding, vectorisation, storing vectors in a database and creating a retriever. In the next section, we use Langchain's document loader as we did previously to load and process our Markdown formatted documents, but this time we use a `VectorstoreIndexCreator` which you can see only requires a couple of parameters - the embedding model that we want to use and the source data (`loader`) to use. However, behind the scenes, the `VectorstoreIndexCreator` is carrying out all of the steps we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b9d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=embeddings_model\n",
    "    ).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75feaab",
   "metadata": {},
   "source": [
    "Now, to run a query against our data, we just need to specify the prompt and then call the index we've created above and pass in the model (`llm`) we want to use and the question we want to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the latest Ant Man movie. When was it released? What is it about?\"\n",
    "index.query(llm=llm, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d70cf",
   "metadata": {},
   "source": [
    "You can see this is a really simple way to implement embeddings and vectors as part of an AI application. It's great for getting up and running quickly.\n",
    "\n",
    "We can use the callback method again to confirm that we're still seeing a reduced number of tokens being consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c04120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain, using the callback to capture the number of tokens used\n",
    "with get_openai_callback() as callback:\n",
    "    index.query(llm=llm, question=query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
